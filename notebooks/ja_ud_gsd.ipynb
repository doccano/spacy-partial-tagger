{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Universal Dependencies (UD) Japanese\n",
        "\n",
        "## Package Installation\n",
        "\n",
        "We would install some required packages on this cell."
      ],
      "metadata": {
        "id": "SXyDhg_2NoPV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lzKJ9zRNDmvF"
      },
      "outputs": [],
      "source": [
        "%pip install -q spacy-partial-tagger conllu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "### Download dataset"
      ],
      "metadata": {
        "id": "5n3JCoeI1aPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -qLO https://raw.githubusercontent.com/megagonlabs/UD_Japanese-GSD/master/spacy/ja_gsd-ud-train.ne.conllu\n",
        "!curl -qLO https://raw.githubusercontent.com/megagonlabs/UD_Japanese-GSD/master/spacy/ja_gsd-ud-dev.ne.conllu\n",
        "!curl -qLO https://raw.githubusercontent.com/megagonlabs/UD_Japanese-GSD/master/spacy/ja_gsd-ud-test.ne.conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OitjLVK0jset",
        "outputId": "f9872987-52f0-43ae-9c19-2446112b077a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 37.8M  100 37.8M    0     0  66.4M      0 --:--:-- --:--:-- --:--:-- 66.4M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2827k  100 2827k    0     0  11.8M      0 --:--:-- --:--:-- --:--:-- 11.7M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2965k  100 2965k    0     0  16.4M      0 --:--:-- --:--:-- --:--:-- 16.4M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation\n",
        "\n",
        "We would convert the CoNLL format dataset to the spaCy format to train a model."
      ],
      "metadata": {
        "id": "bhg6oTApN0fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from conllu import parse_incr\n",
        "from spacy.tokens import Doc, DocBin\n",
        "\n",
        "from spacy_partial_tagger.util import make_char_based_doc\n",
        "\n",
        "\n",
        "def make_doc_bin(vocab: spacy.Vocab, filename: str, max_size: int = 100) -> DocBin:\n",
        "    with open(filename) as f:\n",
        "        dataset = []\n",
        "        for data in parse_incr(f):\n",
        "            tokens = []\n",
        "            tags = []\n",
        "            for x in data:\n",
        "                tokens.append(x[\"form\"])\n",
        "                tags.append(x[\"misc\"].get(\"NE\", \"O\"))\n",
        "            dataset.append((tokens, tags))\n",
        "\n",
        "    db = DocBin()\n",
        "    for tokens, tags in dataset:\n",
        "        doc = Doc(vocab, tokens, spaces=[False] * len(tokens))\n",
        "        char_doc = make_char_based_doc(doc, tags)\n",
        "        if len(char_doc) <= max_size:\n",
        "            db.add(char_doc)\n",
        "        else:\n",
        "            for ent in char_doc.ents:\n",
        "                rest = max_size - len(ent)\n",
        "                start = max(0, ent.start - rest // 2)\n",
        "                end = min(len(char_doc), ent.end + rest // 2)\n",
        "                db.add(char_doc[start:end].as_doc())\n",
        "    return db\n",
        "\n",
        "\n",
        "nlp = spacy.blank(\"ja\")\n",
        "\n",
        "make_doc_bin(nlp.vocab, \"ja_gsd-ud-train.ne.conllu\", 30).to_disk(\"train.spacy\")\n",
        "make_doc_bin(nlp.vocab, \"ja_gsd-ud-dev.ne.conllu\", 1 << 60).to_disk(\"dev.spacy\")\n",
        "make_doc_bin(nlp.vocab, \"ja_gsd-ud-test.ne.conllu\", 1 << 60).to_disk(\"test.spacy\")"
      ],
      "metadata": {
        "id": "i29vxZF8ibjX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "We would train our tagger on the dataset above. First we would create a config file and then train the model based on the config file.\n",
        "\n",
        "### Setup config"
      ],
      "metadata": {
        "id": "Ow3q8_4703gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.cfg\n",
        "[paths]\n",
        "train = \"./train.spacy\"\n",
        "dev = \"./dev.spacy\"\n",
        "init_tok2vec = null\n",
        "vectors = null\n",
        "\n",
        "[corpora]\n",
        "\n",
        "[corpora.train]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.train}\n",
        "max_length = 0\n",
        "\n",
        "[corpora.dev]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.dev}\n",
        "max_length = ${corpora.train.max_length}\n",
        "\n",
        "[system]\n",
        "gpu_allocator = null\n",
        "seed = 0\n",
        "\n",
        "[nlp]\n",
        "lang = \"ja\"\n",
        "pipeline = [\"partial_ner\"]\n",
        "tokenizer = {\"@tokenizers\": \"character_tokenizer.v1\"}\n",
        "disabled = []\n",
        "before_creation = null\n",
        "after_creation = null\n",
        "after_pipeline_creation = null\n",
        "batch_size = 15\n",
        "\n",
        "[nlp.tokenizer]\n",
        "\n",
        "[components]\n",
        "\n",
        "[components.partial_ner]\n",
        "factory = \"partial_ner\"\n",
        "\n",
        "[components.partial_ner.loss]\n",
        "@losses = \"spacy-partial-tagger.ExpectedEntityRatioLoss.v1\"\n",
        "padding_index = -1\n",
        "unknown_index = -100\n",
        "outside_index = 0\n",
        "\n",
        "[components.partial_ner.label_indexer]\n",
        "@label_indexers = \"spacy-partial-tagger.TransformerLabelIndexer.v1\"\n",
        "padding_index = ${components.partial_ner.loss.padding_index}\n",
        "unknown_index= ${components.partial_ner.loss.unknown_index}\n",
        "\n",
        "[components.partial_ner.model]\n",
        "@architectures = \"spacy-partial-tagger.PartialTagger.v1\"\n",
        "\n",
        "[components.partial_ner.model.misaligned_tok2vec]\n",
        "@architectures = \"spacy-partial-tagger.MisalignedTok2VecTransformer.v1\"\n",
        "model_name = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
        "\n",
        "[components.partial_ner.model.encoder]\n",
        "@architectures = \"spacy-partial-tagger.LinearCRFEncoder.v1\"\n",
        "nI = 768\n",
        "nO = null\n",
        "dropout = 0.2\n",
        "\n",
        "[components.partial_ner.model.decoder]\n",
        "@architectures = \"spacy-partial-tagger.ConstrainedViterbiDecoder.v1\"\n",
        "padding_index = ${components.partial_ner.loss.padding_index}\n",
        "\n",
        "[training]\n",
        "dev_corpus = \"corpora.dev\"\n",
        "train_corpus = \"corpora.train\"\n",
        "seed = ${system.seed}\n",
        "gpu_allocator = ${system.gpu_allocator}\n",
        "accumulate_gradient = 1\n",
        "max_steps = 12000\n",
        "patience = 3000\n",
        "eval_frequency = 600\n",
        "frozen_components = []\n",
        "before_to_disk = null\n",
        "\n",
        "[training.batcher]\n",
        "@batchers = \"spacy.batch_by_sequence.v1\"\n",
        "size = 15\n",
        "get_length = null\n",
        "\n",
        "[training.logger]\n",
        "@loggers = \"spacy.ConsoleLogger.v1\"\n",
        "progress_bar = false\n",
        "\n",
        "[training.optimizer]\n",
        "@optimizers = \"Adam.v1\"\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "L2_is_weight_decay = false\n",
        "use_averages = false\n",
        "grad_clip = 5.0\n",
        "\n",
        "[training.optimizer.learn_rate]\n",
        "@schedules = \"slanted_triangular.v1\"\n",
        "max_rate = 0.00002\n",
        "num_steps = ${training.max_steps}\n",
        "cut_frac = 0.1\n",
        "ratio = 16\n",
        "t = -1\n",
        "\n",
        "[training.score_weights]\n",
        "ents_per_type = null\n",
        "ents_f = 1.0\n",
        "ents_p = 0.0\n",
        "ents_r = 0.0\n",
        "\n",
        "[pretraining]\n",
        "\n",
        "[initialize]\n",
        "\n",
        "[initialize.components]\n",
        "\n",
        "[initialize.tokenizer]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9YxLI7AkmF5",
        "outputId": "6dd96619-5b90-47fb-e00f-85dc6fbf7a1a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "To train the model, please execute the command below."
      ],
      "metadata": {
        "id": "e-mbVfL_1zED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run -m spacy train config.cfg \\\n",
        "        --output=./ja-gsd \\\n",
        "        --paths.train train.spacy --paths.dev dev.spacy \\\n",
        "        --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoGNM2CxlSAL",
        "outputId": "495ba4a4-fb26-41d5-d7c9-4fd16f9251a1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[2023-01-18 04:49:08,723] [INFO] Set up nlp object from config\n",
            "INFO:spacy:Set up nlp object from config\n",
            "[2023-01-18 04:49:08,748] [INFO] Pipeline: ['partial_ner']\n",
            "INFO:spacy:Pipeline: ['partial_ner']\n",
            "[2023-01-18 04:49:08,758] [INFO] Created vocabulary\n",
            "INFO:spacy:Created vocabulary\n",
            "[2023-01-18 04:49:08,763] [INFO] Finished initializing nlp object\n",
            "INFO:spacy:Finished initializing nlp object\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Created output directory: ja-gsd\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: ja-gsd\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2023-01-18 04:49:12,472] [INFO] Initialized pipeline components: ['partial_ner']\n",
            "INFO:spacy:Initialized pipeline components: ['partial_ner']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['partial_ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 1.25e-06\u001b[0m\n",
            "E    #       LOSS PARTI...  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  ------  ------  ------  ------\n",
            "  0       0          34.53    0.12    0.06    0.75    0.00\n",
            "  0     600       38287.34   62.38   59.72   65.28    0.62\n",
            "  1    1200       49377.57   73.45   67.62   80.38    0.73\n",
            "  2    1800       21882.08   74.97   68.43   82.89    0.75\n",
            "  3    2400       38962.85   75.44   72.51   78.62    0.75\n",
            "  3    3000       38765.40   73.16   68.23   78.87    0.73\n",
            "  4    3600       25548.66   75.54   76.52   74.59    0.76\n",
            "  5    4200       47331.82   75.13   75.80   74.47    0.75\n",
            "  6    4800       34199.46   75.10   79.30   71.32    0.75\n",
            "  6    5400       25445.73   74.21   76.33   72.20    0.74\n",
            "  7    6000       25420.56   72.68   73.96   71.45    0.73\n",
            "  8    6600       38484.00   70.85   73.03   68.81    0.71\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "ja-gsd/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "We would evaluate the trained model above. Please execute the command below."
      ],
      "metadata": {
        "id": "CUUTTZbP148K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%run -m spacy evaluate ja-gsd/model-best test.spacy --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUJ2V3eW14HO",
        "outputId": "a2d45138-b30d-4e24-a3c6-ffb6d247a668"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "================================== Results ==================================\u001b[0m\n",
            "\n",
            "NER P   75.41\n",
            "NER R   75.53\n",
            "NER F   75.47\n",
            "SPEED   3676 \n",
            "\n",
            "\u001b[1m\n",
            "=============================== NER (per type) ===============================\u001b[0m\n",
            "\n",
            "                   P        R        F\n",
            "NORP           70.37    67.86    69.09\n",
            "PERSON         78.64    91.01    84.38\n",
            "ORG            72.58    60.00    65.69\n",
            "DATE           88.16    79.76    83.75\n",
            "GPE            73.47    87.80    80.00\n",
            "TITLE_AFFIX    71.43    75.00    73.17\n",
            "WORK_OF_ART    82.35    77.78    80.00\n",
            "QUANTITY       78.05    82.05    80.00\n",
            "EVENT          57.14    28.57    38.10\n",
            "ORDINAL        63.16    92.31    75.00\n",
            "PRODUCT        37.50    52.17    43.64\n",
            "FAC            66.67    40.00    50.00\n",
            "MONEY          87.50   100.00    93.33\n",
            "TIME           76.92    76.92    76.92\n",
            "LOC            90.48    76.00    82.61\n",
            "PERCENT        75.00    42.86    54.55\n",
            "LANGUAGE      100.00   100.00   100.00\n",
            "MOVEMENT      100.00    50.00    66.67\n",
            "LAW             0.00     0.00     0.00\n",
            "\n"
          ]
        }
      ]
    }
  ]
}