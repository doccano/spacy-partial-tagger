{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As9I33d9y8TN"
      },
      "source": [
        "# Biomedical named entity recognition with partially annotated data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhyghFvQzD_m"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fMNDuFWzFD1"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKHc9qTuy5_j"
      },
      "outputs": [],
      "source": [
        "!pip install -q spacy-partial-tagger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-md9GFbzNOm"
      },
      "source": [
        "### Download datasets\n",
        "\n",
        "Download biomedical named entity recognition datasets from:\n",
        "- https://github.com/PierreZweigenbaum/bc5cdr-ner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I41coJf5zONs",
        "outputId": "4d9548dd-9086-42ab-acea-4a22cd6c9d74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-10-25 06:34:40--  https://raw.githubusercontent.com/PierreZweigenbaum/bc5cdr-ner/main/BC5CDR-IOB/train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1039940 (1016K) [text/plain]\n",
            "Saving to: ‘train.tsv’\n",
            "\n",
            "train.tsv           100%[===================>]   1016K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-10-25 06:34:41 (17.7 MB/s) - ‘train.tsv’ saved [1039940/1039940]\n",
            "\n",
            "--2022-10-25 06:34:41--  https://raw.githubusercontent.com/PierreZweigenbaum/bc5cdr-ner/main/BC5CDR-IOB/devel.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1031781 (1008K) [text/plain]\n",
            "Saving to: ‘devel.tsv’\n",
            "\n",
            "devel.tsv           100%[===================>]   1008K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-10-25 06:34:42 (17.5 MB/s) - ‘devel.tsv’ saved [1031781/1031781]\n",
            "\n",
            "--2022-10-25 06:34:42--  https://raw.githubusercontent.com/PierreZweigenbaum/bc5cdr-ner/main/BC5CDR-IOB/test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1080717 (1.0M) [text/plain]\n",
            "Saving to: ‘test.tsv’\n",
            "\n",
            "test.tsv            100%[===================>]   1.03M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-10-25 06:34:43 (17.4 MB/s) - ‘test.tsv’ saved [1080717/1080717]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/PierreZweigenbaum/bc5cdr-ner/main/BC5CDR-IOB/train.tsv\n",
        "!wget https://raw.githubusercontent.com/PierreZweigenbaum/bc5cdr-ner/main/BC5CDR-IOB/devel.tsv\n",
        "!wget https://raw.githubusercontent.com/PierreZweigenbaum/bc5cdr-ner/main/BC5CDR-IOB/test.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDe6OQenzqX5"
      },
      "source": [
        "In addition to the datasets, we need to download a dictionary which contains biomedical entities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BydLioNDz0vU",
        "outputId": "5687fb58-b0dc-48a7-c7ef-e01b9216add8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-10-25 06:34:45--  https://raw.githubusercontent.com/shangjingbo1226/AutoNER/master/data/BC5CDR/dict_core.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67944 (66K) [text/plain]\n",
            "Saving to: ‘dict_core.txt’\n",
            "\n",
            "dict_core.txt       100%[===================>]  66.35K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-10-25 06:34:46 (4.31 MB/s) - ‘dict_core.txt’ saved [67944/67944]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/shangjingbo1226/AutoNER/master/data/BC5CDR/dict_core.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36qyvoW4z5W9"
      },
      "source": [
        "## Prepare datasets\n",
        "\n",
        "Once we download the datasets, we need to create a partially annotated dataset and convert datasets into spaCy format. The following code will do everything for you:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RgPwwrNHz6mp"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc, DocBin\n",
        "from spacy_partial_tagger.tokenizer import CharacterTokenizer\n",
        "\n",
        "\n",
        "def preprocess_term(term: str):\n",
        "    chars = []\n",
        "    marks = {\"-\", \"[\", \"]\", \"(\", \")\", \",\", \".\", \"'\"}\n",
        "    for i in range(len(term) - 1):\n",
        "        chars.append(term[i])\n",
        "        if term[i] in marks and term[i + 1] != \" \":\n",
        "            chars.append(\" \")\n",
        "        if term[i] != \" \" and term[i + 1] in marks:\n",
        "            chars.append(\" \")\n",
        "    chars.append(term[-1])\n",
        "    return \"\".join(chars).replace(\"  \", \" \").lower()\n",
        "\n",
        "\n",
        "def load_patterns(file_path: str, nlp):\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            label, term = line.strip().split(\"\\t\")\n",
        "            term = preprocess_term(term)\n",
        "            pattern = [{\"LOWER\": token.text} for token in nlp(term)]\n",
        "            yield {\"label\": label, \"pattern\": pattern}\n",
        "\n",
        "\n",
        "def load_conll(file_path: str):\n",
        "    x, y = [], []\n",
        "    words, tags = [], []\n",
        "    with open(file_path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                word, tag = line.split(\"\\t\")\n",
        "                words.append(word)\n",
        "                tags.append(tag)\n",
        "            else:\n",
        "                x.append(words)\n",
        "                y.append(tags)\n",
        "                words, tags = [], []\n",
        "    if words:\n",
        "        x.append(words)\n",
        "        y.append(tags)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def create_doc(x, y, nlp, set_ents=True):\n",
        "    for words, ents in zip(x, y):\n",
        "        spaces = [True] * len(words)\n",
        "        if not set_ents:\n",
        "            ents = None\n",
        "        yield Doc(nlp.vocab, words=words, spaces=spaces, ents=ents)\n",
        "\n",
        "\n",
        "def store_data(docs, nlp, path: str):\n",
        "    doc_bin = DocBin()\n",
        "    for doc in docs:\n",
        "        ents = [ent for ent in doc.ents]\n",
        "        doc = nlp.make_doc(doc.text)\n",
        "        ents = [\n",
        "            doc.char_span(ent.start_char, ent.end_char, label=ent.label_)\n",
        "            for ent in ents\n",
        "        ]\n",
        "        doc.ents = ents\n",
        "        doc_bin.add(doc)\n",
        "    doc_bin.to_disk(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLilulx40ZSD"
      },
      "source": [
        "Let's use the functions to prepare datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YhsSn8-X0bfW"
      },
      "outputs": [],
      "source": [
        "!mkdir -p corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nJ1gFaIB0Pam"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# set patterns\n",
        "patterns = list(load_patterns(\"dict_core.txt\", nlp))\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "# load data\n",
        "x_train, y_train = load_conll(\"train.tsv\")\n",
        "x_valid, y_valid = load_conll(\"devel.tsv\")\n",
        "x_test, y_test = load_conll(\"test.tsv\")\n",
        "\n",
        "# create docs\n",
        "docs_train = create_doc(x_train, y_train, nlp, set_ents=False)\n",
        "docs_valid = create_doc(x_valid, y_valid, nlp)\n",
        "docs_test = create_doc(x_test, y_test, nlp)\n",
        "docs_train = list(map(ruler, docs_train))\n",
        "\n",
        "# store data\n",
        "nlp.tokenizer = CharacterTokenizer(nlp.vocab)\n",
        "store_data(docs_train, nlp, \"corpus/train.spacy\")\n",
        "store_data(docs_valid, nlp, \"corpus/valid.spacy\")\n",
        "store_data(docs_test, nlp, \"corpus/test.spacy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P1D0PcF0szG"
      },
      "source": [
        "## Creating a config file\n",
        "\n",
        "Let's create a configuration file to train spacy-partial-tagger. Since we are using a dataset from the medical domain, let's use [PubMedBERT](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext) as the pre-trained model.\n",
        "\n",
        "FYI:\n",
        "- https://github.com/doccano/spacy-partial-tagger/blob/main/config.cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM59JGzn1hgw",
        "outputId": "2c75d993-da89-48a6-90ce-c9060cae4a9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing base_config.cfg\n"
          ]
        }
      ],
      "source": [
        "%%writefile base_config.cfg\n",
        "[paths]\n",
        "train = \"./train.spacy\"\n",
        "dev = \"./dev.spacy\"\n",
        "init_tok2vec = null\n",
        "vectors = null\n",
        "\n",
        "[corpora]\n",
        "\n",
        "[corpora.train]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.train}\n",
        "\n",
        "[corpora.dev]\n",
        "@readers = \"spacy.Corpus.v1\"\n",
        "path = ${paths.dev}\n",
        "\n",
        "[system]\n",
        "gpu_allocator = null\n",
        "seed = 0\n",
        "\n",
        "[nlp]\n",
        "lang = \"en\"\n",
        "pipeline = [\"partial_ner\"]\n",
        "batch_size = 16\n",
        "tokenizer = {\"@tokenizers\": \"character_tokenizer.v1\"}\n",
        "\n",
        "[nlp.tokenizer]\n",
        "\n",
        "[components]\n",
        "\n",
        "[components.partial_ner]\n",
        "factory = \"partial_ner\"\n",
        "\n",
        "[components.partial_ner.loss]\n",
        "@losses = \"spacy-partial-tagger.ExpectedEntityRatioLoss.v1\"\n",
        "padding_index = -1\n",
        "unknown_index = -100\n",
        "outside_index = 0\n",
        "\n",
        "[components.partial_ner.label_indexer]\n",
        "@label_indexers = \"spacy-partial-tagger.TransformerLabelIndexer.v1\"\n",
        "padding_index = ${components.partial_ner.loss.padding_index}\n",
        "unknown_index= ${components.partial_ner.loss.unknown_index}\n",
        "\n",
        "[components.partial_ner.model]\n",
        "@architectures = \"spacy-partial-tagger.PartialTagger.v1\"\n",
        "\n",
        "[components.partial_ner.model.misaligned_tok2vec]\n",
        "@architectures = \"spacy-partial-tagger.MisalignedTok2VecTransformer.v1\"\n",
        "model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
        "\n",
        "[components.partial_ner.model.encoder]\n",
        "@architectures = \"spacy-partial-tagger.LinearCRFEncoder.v1\"\n",
        "nI = 768\n",
        "nO = null\n",
        "dropout = 0.2\n",
        "\n",
        "[components.partial_ner.model.decoder]\n",
        "@architectures = \"spacy-partial-tagger.ConstrainedViterbiDecoder.v1\"\n",
        "padding_index = ${components.partial_ner.loss.padding_index}\n",
        "\n",
        "[training]\n",
        "dev_corpus = \"corpora.dev\"\n",
        "train_corpus = \"corpora.train\"\n",
        "seed = ${system.seed}\n",
        "gpu_allocator = ${system.gpu_allocator}\n",
        "accumulate_gradient = 1\n",
        "max_steps = 20000\n",
        "patience = 10000\n",
        "eval_frequency = 1000\n",
        "frozen_components = []\n",
        "before_to_disk = null\n",
        "\n",
        "[training.batcher]\n",
        "@batchers = \"spacy.batch_by_sequence.v1\"\n",
        "size = 16\n",
        "get_length = null\n",
        "\n",
        "[training.logger]\n",
        "@loggers = \"spacy.ConsoleLogger.v1\"\n",
        "progress_bar = false\n",
        "\n",
        "[training.optimizer]\n",
        "@optimizers = \"Adam.v1\"\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "L2_is_weight_decay = false\n",
        "use_averages = false\n",
        "grad_clip = 5.0\n",
        "\n",
        "[training.optimizer.learn_rate]\n",
        "@schedules = \"slanted_triangular.v1\"\n",
        "max_rate = 0.00002\n",
        "num_steps = ${training.max_steps}\n",
        "cut_frac = 0.1\n",
        "ratio = 16\n",
        "t = -1\n",
        "\n",
        "[training.score_weights]\n",
        "ents_per_type = null\n",
        "ents_f = 1.0\n",
        "ents_p = 0.0\n",
        "ents_r = 0.0\n",
        "\n",
        "[pretraining]\n",
        "\n",
        "[initialize]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbK4Mcx63CCW"
      },
      "outputs": [],
      "source": [
        "!python -m spacy init fill-config base_config.cfg config.cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdRp3gVb1tYj"
      },
      "source": [
        "## Training a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqO6msuX1uOY"
      },
      "outputs": [],
      "source": [
        "!python -m spacy train config.cfg \\\n",
        "        --output=./pubmed \\\n",
        "        --paths.train corpus/train.spacy \\\n",
        "        --paths.dev corpus/valid.spacy \\\n",
        "        --gpu-id 0 \\\n",
        "        --training.patience 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa1zA56s18AY"
      },
      "source": [
        "## Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNBAQK1m1_ug"
      },
      "outputs": [],
      "source": [
        "!python -m spacy evaluate pubmed/model-best corpus/test.spacy --gpu-id 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtZQ0sTR2AK4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
